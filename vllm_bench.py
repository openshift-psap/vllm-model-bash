#!/usr/bin/env python3
"""
vllm_bench.py - Scenario-based vLLM benchmarking tool
"""

import argparse
import csv
import json
import os
import shutil
import signal
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests
import yaml


class VLLMBenchmark:
    def __init__(self, config_path: str, scenario_filter: Optional[List[str]] = None):
        self.config_path = Path(config_path)
        self.scenario_filter = scenario_filter
        self.config = self._load_config()
        self.study_dir = self._setup_study_dir()
        self.summary_data = []
        self.server_process = None

    def _load_config(self) -> Dict[str, Any]:
        """Load and validate scenario-based config."""
        print(f"üìÑ Loading config: {self.config_path}")

        with open(self.config_path) as f:
            config = yaml.safe_load(f)

        # Validate
        if 'model' not in config or 'name' not in config['model']:
            raise ValueError("Invalid config: missing 'model.name'")

        if 'scenarios' not in config or not config['scenarios']:
            raise ValueError("No scenarios found in config")

        # Filter scenarios
        scenarios = config['scenarios']
        if self.scenario_filter:
            scenarios = [s for s in scenarios if s.get('name') in self.scenario_filter]
            if not scenarios:
                raise ValueError(f"No scenarios matched filter: {self.scenario_filter}")
            print(f"üîç Filtered scenarios: {[s['name'] for s in scenarios]}")

        config['scenarios'] = scenarios
        return config

    def _setup_study_dir(self) -> Path:
        """Create study directory structure."""
        model_name = self.config['model']['name'].replace('/', '_')
        defaults = self.config.get('defaults', {})
        study_name = defaults.get('study_dir', f'Study_{model_name}')

        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        study_dir = Path(f"{study_name}_{timestamp}")
        study_dir.mkdir(parents=True, exist_ok=True)

        print(f"üìÅ Study directory: {study_dir}")

        # Copy config
        shutil.copy(self.config_path, study_dir / 'config.yaml')

        return study_dir

    def _apply_env_vars(self, env_dict: Dict[str, str]):
        """Apply environment variables."""
        if not env_dict:
            return

        print("üåç Environment variables:")
        for key, value in env_dict.items():
            os.environ[key] = str(value)
            print(f"  {key}={value}")

    def _wait_for_server(self, port: int, timeout: int = 120) -> bool:
        """Wait for vLLM server to be ready."""
        url = f"http://localhost:{port}/v1/models"
        print(f"‚è≥ Waiting for server on port {port}...")

        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(url, timeout=2)
                if response.status_code == 200:
                    print("‚úÖ Server ready")
                    return True
            except requests.exceptions.RequestException:
                pass
            time.sleep(2)

        print("‚ùå Server failed to start (timeout)")
        return False

    def _get_nsys_session_id(self) -> Optional[str]:
        """Get the active nsys session ID using awk.

        Expected output format:
        ID         TIME                                      STATE LAUNCH NAME
        <session_id> <timestamp>                            <state> <name>
        """
        try:
            # Use awk to skip header and extract first column (session ID)
            # NR==2 means "line number 2" (first data line after header)
            result = subprocess.run(
                "nsys sessions list | awk 'NR==2 {print $1}'",
                shell=True,
                capture_output=True,
                text=True,
                timeout=5
            )

            session_id = result.stdout.strip()
            if session_id:
                return session_id

        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to get nsys session ID: {e}")

        return None

    def _start_server(
        self,
        scenario_name: str,
        port: int,
        params: List[str],
        log_file: Path,
        nsys_profile: bool = False,
        nsys_args: str = ""
    ) -> tuple[Optional[subprocess.Popen], Optional[str]]:
        """Start vLLM server. Returns (process, nsys_session_id)."""
        model_name = self.config['model']['name']

        use_session_mode = False
        cmd = []

        if nsys_profile:
            nsys_launch_args = nsys_args or "--trace=cuda,nvtx,osrt"
            # Check if using --start-later (session mode)
            use_session_mode = "--start-later=true" in nsys_launch_args or "--start-later true" in nsys_launch_args

            if use_session_mode:
                print("üîß Using nsys session mode (start-later)")

            cmd = ["nsys", "profile"] + nsys_launch_args.split()

        cmd += ["vllm", "serve", model_name, "--port", str(port)] + params

        print(f"‚ñ∂Ô∏è  Starting server: {' '.join(cmd[:5])}...")

        log_file.parent.mkdir(parents=True, exist_ok=True)
        with open(log_file, 'w') as f:
            process = subprocess.Popen(
                cmd,
                stdout=f,
                stderr=subprocess.STDOUT,
                preexec_fn=os.setsid
            )

        print(f"üîë PID: {process.pid}")

        # Get session ID if using session mode
        nsys_session_id = None
        if use_session_mode:
            # Wait a moment for nsys to initialize
            time.sleep(2)
            nsys_session_id = self._get_nsys_session_id()
            if nsys_session_id:
                print(f"üìä Nsys session ID: {nsys_session_id}")
            else:
                print("‚ö†Ô∏è  Could not retrieve nsys session ID")

        return process, nsys_session_id

    def _stop_server(self, process: subprocess.Popen):
        """Stop vLLM server gracefully."""
        if not process:
            return

        print(f"üõë Stopping server (PID: {process.pid})")
        try:
            os.killpg(os.getpgid(process.pid), signal.SIGTERM)
        except ProcessLookupError:
            pass

        try:
            process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            os.killpg(os.getpgid(process.pid), signal.SIGKILL)

        time.sleep(2)

    def _run_benchmark(
        self,
        scenario_name: str,
        port: int,
        concurrency: int,
        num_prompts: int,
        input_len: int,
        output_len: int,
        result_file: Path,
        enable_profiling: bool = False
    ) -> tuple[str, int]:
        """Run vLLM benchmark."""
        model_name = self.config['model']['name']

        cmd = [
            "vllm", "bench", "serve",
            "--base-url", f"http://localhost:{port}",
            "--model", model_name,
            "--dataset-name", "random",
            "--random-input-len", str(input_len),
            "--random-output-len", str(output_len),
            "--max-concurrency", str(concurrency),
            "--num-prompts", str(num_prompts),
            "--seed", str(int(time.time())),
            "--save-result",
            "--result-filename", str(result_file),
            "--append-result"
        ]

        if enable_profiling:
            cmd.append("--profile")

        print(f"===== Concurrency: {concurrency} ({num_prompts} prompts) =====")

        start_time = time.time()
        try:
            subprocess.run(cmd, check=True, capture_output=False)
            status = "success"
        except subprocess.CalledProcessError:
            status = "failed"

        runtime = int(time.time() - start_time)
        return status, runtime

    def _run_scenario(self, scenario: Dict):
        """Run a single scenario."""
        scenario_name = scenario['name']
        print("\n" + "=" * 50)
        print(f"üìã Scenario: {scenario_name}")
        print("=" * 50)

        # Setup paths
        scenario_dir = self.study_dir / f"scenario_{scenario_name}"
        scenario_dir.mkdir(parents=True, exist_ok=True)
        (scenario_dir / 'logs').mkdir(exist_ok=True)
        (scenario_dir / 'results').mkdir(exist_ok=True)
        (scenario_dir / 'profiles').mkdir(exist_ok=True)

        log_file = scenario_dir / 'logs' / 'vllm_server.log'

        # Merge parameters
        base_params = self.config['model'].get('base_params', '')
        scenario_params = scenario.get('params', '')
        full_params = f"{base_params} {scenario_params}".strip().split()

        # Handle compilation config
        if 'compilation_config' in scenario:
            comp_config = scenario['compilation_config']
            if isinstance(comp_config, str):
                comp_config = yaml.safe_load(comp_config)
            full_params.extend(['--compilation-config', json.dumps(comp_config)])

        port = scenario.get('port', 8000)
        print(f"üåê Port: {port}")
        print(f"‚öôÔ∏è  Params: {' '.join(full_params[:3])}...")

        # Get benchmark settings
        defaults = self.config.get('defaults', {})
        bench_defaults = defaults.get('bench', {})
        bench_config = {**bench_defaults, **scenario.get('bench', {})}

        concurrencies = bench_config.get('concurrencies', [1, 32, 128])
        input_len = bench_config.get('input_len', 2000)
        output_len = bench_config.get('output_len', 200)
        cc_mult = bench_config.get('cc_mult', 10)
        result_prefix = bench_config.get('result_prefix', f"scenario_{scenario_name}")

        result_file = scenario_dir / 'results' / f"{result_prefix}.json"

        # Profiling settings
        profile_nsys = scenario.get('profile', False)
        nsys_launch_args = scenario.get('profiling', {}).get('nsys_launch_args', '')
        torch_enabled = scenario.get('profiling', {}).get('torch_profiler', {}).get('enabled', False)

        # Setup torch profiler environment BEFORE launching server
        torch_dir = None
        if torch_enabled:
            torch_dir = scenario_dir / 'profiles' / 'torch'
            torch_dir.mkdir(parents=True, exist_ok=True)

            torch_config = scenario.get('profiling', {}).get('torch_profiler', {})
            print(f"üî• Torch profiler enabled ‚Üí {torch_dir}")

            os.environ['VLLM_TORCH_PROFILER_DIR'] = str(torch_dir)
            os.environ['VLLM_TORCH_PROFILER_RECORD_SHAPES'] = str(torch_config.get('record_shapes', True)).lower()
            os.environ['VLLM_TORCH_PROFILER_PROFILE_MEMORY'] = str(torch_config.get('profile_memory', True)).lower()
            os.environ['VLLM_TORCH_PROFILER_WITH_STACK'] = str(torch_config.get('with_stack', False)).lower()
            os.environ['VLLM_TORCH_PROFILER_WITH_FLOPS'] = str(torch_config.get('with_flops', False)).lower()

        # Start server
        self.server_process, nsys_session_id = self._start_server(
            scenario_name, port, full_params, log_file,
            nsys_profile=profile_nsys, nsys_args=nsys_launch_args
        )

        if not self.server_process or not self._wait_for_server(port):
            print(f"‚ùå Failed to start server for scenario: {scenario_name}")
            if self.server_process:
                self._stop_server(self.server_process)
            return

        # Run benchmarks for each concurrency
        for concurrency in concurrencies:
            num_prompts = concurrency * cc_mult

            # Update torch profiler prefix for this concurrency
            if torch_enabled:
                os.environ['VLLM_TORCH_PROFILER_PREFIX'] = f"trace_conc{concurrency}"

            # Start nsys profiling
            nsys_file = None
            if profile_nsys:
                nsys_file = scenario_dir / 'profiles' / f"nsys_conc{concurrency}"
                nsys_start_args = scenario.get('profiling', {}).get('nsys_start_args', '--force-overwrite=true')

                if nsys_session_id:
                    # Session-based profiling
                    print(f"üé• nsys start --session={nsys_session_id} ‚Üí {nsys_file}.qdrep")
                    cmd = ['nsys', 'start', f'--session={nsys_session_id}']
                    cmd += nsys_start_args.split() + ['--output', str(nsys_file)]
                    subprocess.run(cmd)
                else:
                    # Legacy mode (no session)
                    print(f"üé• nsys start ‚Üí {nsys_file}.qdrep")
                    subprocess.run(['nsys', 'start'] + nsys_start_args.split() + ['--output', str(nsys_file)])

            # Run benchmark
            status, runtime = self._run_benchmark(
                scenario_name, port, concurrency, num_prompts,
                input_len, output_len, result_file,
                enable_profiling=torch_enabled
            )

            # Stop nsys profiling
            if profile_nsys:
                if nsys_session_id:
                    print(f"üõë nsys stop --session={nsys_session_id}")
                    subprocess.run(['nsys', 'stop', f'--session={nsys_session_id}'])
                else:
                    print("üõë nsys stop")
                    subprocess.run(['nsys', 'stop'])
                print(f"‚úÖ Saved: {nsys_file}.qdrep")

            # Report torch profiler output
            if torch_dir:
                print(f"‚úÖ Torch traces: {torch_dir}/")
                trace_files = list(torch_dir.glob(f"trace_conc{concurrency}*"))
                for f in trace_files:
                    print(f"    {f.name} ({f.stat().st_size} bytes)")

            # Record summary
            self.summary_data.append({
                'scenario': scenario_name,
                'port': port,
                'concurrency': concurrency,
                'num_prompts': num_prompts,
                'status': status,
                'runtime_sec': runtime,
                'result_file': str(result_file)
            })

        # Stop server
        self._stop_server(self.server_process)

    def run(self):
        """Run all scenarios."""
        print("üöÄ vLLM Scenario Benchmark")
        print(f"üéØ Model: {self.config['model']['name']}")
        print(f"üìä Scenarios: {len(self.config['scenarios'])}")

        # Apply global environment variables
        env_defaults = self.config.get('defaults', {}).get('env', {})
        self._apply_env_vars(env_defaults)

        # Setup summary file
        summary_file = self.study_dir / 'summary.csv'
        with open(summary_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'scenario', 'port', 'concurrency', 'num_prompts',
                'status', 'runtime_sec', 'result_file'
            ])
            writer.writeheader()

        # Run each scenario
        try:
            for scenario in self.config['scenarios']:
                self._run_scenario(scenario)
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Interrupted by user")
            if self.server_process:
                self._stop_server(self.server_process)
        except Exception as e:
            print(f"‚ùå Error: {e}")
            if self.server_process:
                self._stop_server(self.server_process)
            raise

        # Write summary
        with open(summary_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=self.summary_data[0].keys())
            writer.writeheader()
            writer.writerows(self.summary_data)

        print("\n" + "=" * 50)
        print("üéâ All scenarios completed!")
        print(f"üìä Summary: {summary_file}")
        print(f"üìÅ Study directory: {self.study_dir}")


def main():
    parser = argparse.ArgumentParser(
        description="vLLM scenario-based benchmark tool",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('config', help='Path to scenario config YAML file')
    parser.add_argument(
        '--scenario', '--scenarios',
        dest='scenarios',
        help='Comma-separated list of scenarios to run'
    )

    args = parser.parse_args()

    scenario_filter = None
    if args.scenarios:
        scenario_filter = [s.strip() for s in args.scenarios.split(',')]

    try:
        benchmark = VLLMBenchmark(args.config, scenario_filter)
        benchmark.run()
    except Exception as e:
        print(f"‚ùå Fatal error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
